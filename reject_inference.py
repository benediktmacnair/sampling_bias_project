# -*- coding: utf-8 -*-
"""Reject Inference 14June

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10WRy0veOjqsm3LcThWvtOXZg1lFJrgIN
"""

from abc import ABC, abstractmethod
import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from sklearn.ensemble import IsolationForest
from sklearn.exceptions import NotFittedError # Import NotFittedError
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score, brier_score_loss, roc_curve
from scipy.stats import multivariate_normal, norm
from scipy.optimize import minimize # Import minimize
import statsmodels.api as sm # Import statsmodels.api
from abc import ABC, abstractmethod
from typing import List, Union
from Evaluation import Metric, AUC, PAUC, BS, ABR, Evaluation, bayesianMetric
import random
import copy


# Define a small epsilon for numerical stability
EPS = 1e-10

class RejectInference(ABC):

    def __init__(self):
        pass

    @abstractmethod
    def fit(self, accepts_x: pd.DataFrame, accepts_y: pd.Series, rejects_x: pd.DataFrame, holdout_x: pd.DataFrame):
        """
        Abstract method to build the reject inference model.

        Parameters
        ----------
        accepts_x : pd.DataFrame
            Features of the accepted (observed) samples.
        accepts_y : pd.Series
            True labels of the accepted samples.
        rejects_x : pd.DataFrame
            Features of the rejected samples whose outcomes are unknown.
            This is the *original full set* of rejected data.
        holdout_x : pd.DataFrame
            Features of the holdout sample, for evaluation purposes.

        Returns
        -------
        self : object
            Returns the instance itself after fitting.
        """
        pass

    @abstractmethod
    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Predict class labels for samples in X using the fitted model.

        Parameters
        ----------
        X : pd.DataFrame
            Features of the samples.

        Returns
        -------
        y_pred : array-like of shape (n_samples,)
            Predicted class labels (0 or 1).

        """
        pass

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """
        Abstract method to predict class probabilities for samples in X.

        Subclasses must implement this method to provide probability
        predictions using their internal fitted model.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            The input samples.

        Returns
        -------
        y_proba : array-like of shape (n_samples, n_classes)
            Predicted class probabilities. For binary classification,
            this is typically an array with two columns (probability of class 0,
            probability of class 1).

        """
        pass

class BiasAwareSelfLearning(RejectInference):

    def __init__(self, strong_estimator: object, weak_learner_estimator: object,
                 max_iterations: int = 3,
                 silent: bool = False,
                 filtering_beta: tuple = (0.05, 1.0),
                 sampling_percent: float = 0.8,
                 holdout_percent: float = 0.1,
                 labeling_percent=0.01,
                 multiplier=2.0,
                 early_stop: bool = False,
                 evaluation_metric: Metric = AUC(),
                 bayesian_acc_rate: List[float] = [0.2, 0.4], # Add parameter for acc_rate used in Bayesian Metric
                 bayesian_fnr_range: List[float] = [0, 0.2]): # Add parameter for fnr_range used in Bayesian Metric
        """
        Parameters
        ----------
        strong_estimator : object
            The 'strong' base estimator for final model training.
        weak_learner_estimator : object
            The 'weak' base estimator for iterative pseudo-labeling.
        max_iterations : int, default=3
            Maximum number of self-learning iterations.
        silent : bool, default=False
            Whether to print training details.
        filtering_beta : tuple, default=(0.05, 1.0)
            Percentiles of rejects to be filtered (e.g., (0.1, 0.9) to keep middle 80%).
        sampling_percent : float, default=0.8
            Percentage of remaining rejects to be sampled in each labeling iteration. Ï in Algorithm B.2
        labeling_percent : float, default=0.01
            Percentage of sampled rejects to be labeled (for the 'bad' class). 	Î³ in Algorithm B.2
            This defines the upper quantile for 'bad' predictions.
        multiplier : float, default=2. Î¸ in Algorithm B.2
            Imbalance multiplier for 'bad' vs 'good' rejects during labeling.
            `per_g = labeling_percent / multiplier` means 'good' labels are `1/multiplier` times less likely to be selected than 'bad' based on quantile.
            We expect the bad rate in reject to be higher than in accept.Therefore,detting ðœƒ >1 helps to append
            more bad than good examples, increasing the bad rate in the training sample to approximate the population distribution.
        early_stop : bool, default=False
            Whether to use performance-based early stopping. If True, the labeling
            iterations will stop if the performance metric on the holdout set
            does not improve.
        evaluation_metric : Metric, default=AUC()
            An instance of a Metric (e.g., AUC, BS, PAUC, ABR) to be used by the
            Bayesian evaluation (BM).
        bayesian_acc_rate : List[float], default=[0.2, 0.4]
            Acceptance rate range for the Bayesian Metric evaluation (used by ABR).
        bayesian_fnr_range : List[float], default=[0, 0.2]
            False Negative Rate range for the Bayesian Metric evaluation (used by PAUC).
        """
        # Call parent constructor first
        super().__init__()

        # Store parameters specific to BiasAwareSelfLearning
        self.strong_estimator = strong_estimator # Stored as strong_estimator
        self.weak_learner_estimator = weak_learner_estimator
        self.max_iterations = max_iterations
        self.silent = silent
        self.filtering_beta = filtering_beta
        self.sampling_percent = sampling_percent
        self.holdout_percent = holdout_percent
        self.labeling_percent = labeling_percent
        self.multiplier = multiplier
        self.early_stop = early_stop
        self.evaluation_metric = evaluation_metric
        self.bayesian_acc_rate = bayesian_acc_rate # Store the bayesian_acc_rate
        self.bayesian_fnr_range = bayesian_fnr_range # Store the bayesian_fnr_range


        # Initialize the Bayesian evaluator instance. It will use its own defaults for BM params.
        # The BM method itself accepts acc_rate and fnr_range, which is handled when BM is called.
        self.bayesian_evaluator = bayesianMetric(metric=self.evaluation_metric)

        # --- Parameter Validations ---

        # Validate strong_estimator
        if self.strong_estimator is None:
            raise ValueError("`strong_estimator` cannot be None.")
        if not hasattr(self.strong_estimator, 'predict_proba'):
            raise AttributeError("Strong learner (`strong_estimator`) must support `predict_proba` method.")

        # Validate weak_learner_estimator
        if self.weak_learner_estimator is None:
            raise ValueError("`weak_learner_estimator` cannot be None.")

        # Validate labeling_percent
        if not isinstance(self.labeling_percent, (int, float)) or not (0 < self.labeling_percent < 1):
            raise ValueError("`labeling_percent` must be a float strictly between 0 and 1.")

        # Validation for multiplier
        if not (self.multiplier > 1):
            raise ValueError("`multiplier` should be a float strictly greater than 1. "
                             "We expect the bad rate in reject to be higher than in accept.")

        # Validate max_iterations
        if not isinstance(self.max_iterations, int) or self.max_iterations < 1:
            raise ValueError("`max_iterations` must be an integer greater than or equal to 1.")

        # Validate silent
        if not isinstance(self.silent, bool):
            raise ValueError("`silent` must be a boolean.")

        # Validate filtering_beta
        if not isinstance(self.filtering_beta, (tuple, list)) or len(self.filtering_beta) != 2:
            raise ValueError("`filtering_beta` must be a tuple or list of two floats.")
        if not all(isinstance(val, (int, float)) for val in self.filtering_beta):
            raise ValueError("Elements of `filtering_beta` must be floats or integers.")
        if not (0.0 <= self.filtering_beta[0] <= 1.0 and 0.0 <= self.filtering_beta[1] <= 1.0):
            raise ValueError("Elements of `filtering_beta` must be between 0.0 and 1.0 (inclusive).")
        if not (self.filtering_beta[0] <= self.filtering_beta[1]):
            raise ValueError("The first element of `filtering_beta` (lower bound) must be less than or equal to the second (upper bound).")

        # Validate sampling_percent
        if not isinstance(self.sampling_percent, (int, float)) or not (0 < self.sampling_percent < 1):
            raise ValueError("`sampling_percent` must be a float strictly between 0 and 1.")

        # Validate holdout_percent
        if not isinstance(self.holdout_percent, (int, float)) or not (0 <= self.holdout_percent < 1):
            raise ValueError("`holdout_percent` must be a float between 0 (inclusive) and 1 (exclusive).")

        # Validate holdout_percent
        if not isinstance(self.holdout_percent, (int, float)) or not (0 <= self.holdout_percent < 1): # Changed to <1, as 1 would mean no training data.
            raise ValueError("`holdout_percent` must be a float between 0 (inclusive) and 1 (exclusive).")

        # Validate early_stop
        if not isinstance(self.early_stop, bool):
            raise ValueError("`early_stop` must be a boolean.")

        # Validate evaluation_metric
        if not isinstance(self.evaluation_metric, Metric):
            raise TypeError("`evaluation_metric` must be an instance of a `Metric` subclass (e.g., AUC, BS, PAUC, ABR).")



    #helper function for filtering, reference to Algorithm B.1 in appendix
    def _filtering_stage_basl(self, accepts_x: pd.DataFrame, rejects_x: pd.DataFrame) -> np.ndarray:
        """
        Filters rejected cases that are most and least similar to accepts using Isolation Forest.
        This function mimics the R `filteringStage` and is local to BiasAwareSelfLearning.

        Parameters
        ----------
        accepts_x : pd.DataFrame
            DataFrame containing accepted applications features.
        rejects_x : pd.DataFrame
            DataFrame containing rejected applications features.

        Returns
        -------
        filtered_rejects_x : pd.DataFrame
            DataFrame containing rejected samples that passed the filter.
        """

        # Fit Isolation Forest on accepted data (treating accepts as 'normal' data)
        model = IsolationForest(n_estimators=100, random_state=7)
        model.fit(accepts_x)

        # Get anomaly scores for rejects.
        scores = -model.decision_function(rejects_x)

        # Filter based on percentiles using self.filtering_beta
        lower_bound = np.percentile(scores, self.filtering_beta[0] * 100)
        upper_bound = np.percentile(scores, self.filtering_beta[1] * 100)

        tmp_rejects_mask = (scores >= lower_bound) & (scores <= upper_bound)

        return rejects_x[tmp_rejects_mask].copy()


    #helper function, reference to Algorithm B.2 in appendix
    def _labeling(self, current_train_X: pd.DataFrame, current_train_y: pd.Series, current_unlabeled_X: pd.DataFrame, iteration_number: int) -> tuple[pd.DataFrame, pd.Series, pd.DataFrame, int]:
        """
        This method samples a subset of the currently unlabeled rejected data, trains a weak
        learner on the combined accepted and previously pseudo-labeled data, and then
        assigns confident pseudo-labels (0 or 1) to the sampled unlabeled instances
        based on defined confidence thresholds (`labeling_percent` and `multiplier`).

        Parameters
        ----------
        current_train_X : pd.DataFrame
            The features of the currently labeled training data (accepted samples +
            previously pseudo-labeled rejected samples).
        current_train_y : pd.Series
            The labels corresponding to `current_train_X`.
        current_unlabeled_X : pd.DataFrame
            The features of the rejected samples that are currently unlabeled and
            available for pseudo-labeling in the current iteration.
        iteration_number : int
            The current iteration number of the self-learning loop, used primarily
            for verbose output and debugging.

        Returns
        -------
        tuple[pd.DataFrame, pd.Series, pd.DataFrame, int]
            - **`updated_train_X`** (`pd.DataFrame`): The augmented training features,
              including the `current_train_X` and the `newly_labeled_features`.
            - **`updated_train_y`** (`pd.Series`): The augmented training labels,
              including the `current_train_y` and the `newly_labeled_y`.
            - **`updated_unlabeled_X`** (`pd.DataFrame`): The remaining unlabeled
              rejected samples after the current iteration's pseudo-labeling.
              These are the samples from `current_unlabeled_X` that were *not* sampled or *not* confidently pseudo-labeled.
            - **`num_pseudo_labeled`** (`int`): The count of samples that were
              confidently pseudo-labeled in this iteration.
        """

        if current_unlabeled_X.empty:
            if not self.silent:
                print(f"Iteration {iteration_number}: No rejected samples remaining for labeling.")
            return current_train_X, current_train_y, current_unlabeled_X, 0

        # Sample a subset of unlabeled data
        n_unlabeled = len(current_unlabeled_X)
        n_sample = max(1, round(self.sampling_percent * n_unlabeled))

        sampled_indices = random.sample(list(current_unlabeled_X.index), n_sample)
        X_sampled = current_unlabeled_X.loc[sampled_indices].copy()

        # Train weak model and predict probabilities
        current_weak_model = copy.deepcopy(self.weak_learner_estimator)
        current_weak_model.fit(current_train_X, current_train_y)
        s_star_proba = current_weak_model.predict_proba(X_sampled)[:, 1]
        s_star_series = pd.Series(s_star_proba, index=X_sampled.index)

        # Determine confidence thresholds
        # con_g: Threshold for confident GOOD (low prob of being BAD)
        con_g = np.quantile(s_star_series, self.labeling_percent / self.multiplier)

        # con_b: Threshold for confident BAD (high prob of being BAD)
        # This is equivalent to (self.labeling_percent * self.multiplier) percentile from the TOP
        con_b = np.quantile(s_star_series, 1 - self.labeling_percent)

        # Identify confident good (0) and bad (1) samples
        confident_good_mask = (s_star_series <= con_g)
        confident_bad_mask = (s_star_series >= con_b)

        # Ensure masks do not overlap to prevent overwriting. Prioritize 'good' if needed, or exclude overlap.
        # For this implementation, we will only label samples that are exclusively in one confident band.
        exclusively_good_mask = confident_good_mask & ~confident_bad_mask
        exclusively_bad_mask = confident_bad_mask & ~confident_good_mask

        # Combine masks for newly labeled features
        confident_total_mask = exclusively_good_mask | exclusively_bad_mask
        newly_labeled_features = X_sampled[confident_total_mask].copy()

        # Assign pseudo-labels
        newly_labeled_y = pd.Series(index=newly_labeled_features.index, dtype=int)
        newly_labeled_y.loc[s_star_series[exclusively_good_mask].index] = 0
        newly_labeled_y.loc[s_star_series[exclusively_bad_mask].index] = 1

        num_pseudo_labeled = len(newly_labeled_y)

        if num_pseudo_labeled == 0:
            if not self.silent:
                print(f"Iteration {iteration_number}: No confident pseudo-labels found.")
            return current_train_X, current_train_y, current_unlabeled_X, 0

        # Augment training data with newly labeled samples
        updated_train_X = pd.concat([current_train_X, newly_labeled_features], axis=0)
        updated_train_y = pd.concat([current_train_y, newly_labeled_y], axis=0)

        # Update master inferred labels
        self.inferred_labels_.loc[newly_labeled_y.index] = newly_labeled_y.values

        # Remove labeled data from unlabeled pool
        updated_unlabeled_X = current_unlabeled_X.drop(index=newly_labeled_features.index, errors='ignore').copy()

        if not self.silent:
            num_bad = sum(newly_labeled_y == 1)
            num_good = sum(newly_labeled_y == 0)
            print(f"Iteration {iteration_number}: Pseudo-labeled {num_pseudo_labeled} cases ({num_bad} BAD + {num_good} GOOD).")
            print(f"Remaining unlabeled samples: {len(updated_unlabeled_X)}")

        return updated_train_X, updated_train_y, updated_unlabeled_X, num_pseudo_labeled


    # With reference to Algorithm B.3 in Appendix
    def fit(self, accepts_x: pd.DataFrame, accepts_y: pd.Series, rejects_x: pd.DataFrame):
        """
        Builds the Bias-Aware Self-Learning model based on the provided pseudo-code blueprint.

        Parameters
        ----------
        accepts_x : pd.DataFrame
            Features of the accepted (observed) samples.
        accepts_y : pd.Series
            True labels (0/1) of the accepted samples.
        rejects_x : pd.DataFrame
            Features of the rejected samples whose outcomes are unknown.
            This is the *original full set* of rejected data.

        Returns
        -------
        self : object
            Returns the instance itself after fitting.
        """

        # --- Input Validation  ---
        if not isinstance(accepts_x, pd.DataFrame):
            raise TypeError("`accepts_x` must be a pandas DataFrame.")
        if not isinstance(accepts_y, pd.Series):
            raise TypeError("`accepts_y` must be a pandas Series.")
        if accepts_x.empty or accepts_y.empty:
            raise ValueError("`accepts_x` and `accepts_y` cannot be empty. They form the initial labeled dataset.")
        if len(accepts_x) != len(accepts_y):
            raise ValueError("`accepts_x` and `accepts_y` must have the same number of rows.")

        if not isinstance(rejects_x, pd.DataFrame):
            raise TypeError("`rejects_x` must be a pandas DataFrame.")
        if rejects_x.empty:
            raise ValueError("`rejects_x` cannot be empty.")


        # Check column consistency across feature dataframes
        feature_cols = accepts_x.columns.tolist()
        if not rejects_x.empty and feature_cols != rejects_x.columns.tolist():
            raise ValueError("Columns of `rejects_x` must match `accepts_x`.")

        # Set random seeds for reproducibility
        random.seed(7)
        np.random.seed(7)

        # Initialize current training data and unlabeled rejects

        current_train_X, holdout_x_accept, current_train_y, holdout_y_accept = train_test_split(
            accepts_x, accepts_y, test_size=self.holdout_percent,stratify=accepts_y)

        current_unlabeled_X, holdout_x_reject = train_test_split(
            rejects_x, test_size=self.holdout_percent)

        self.inferred_labels_ = pd.Series(index=rejects_x.index, dtype=int).fillna(-1)

        # Apply filtering stage
        current_unlabeled_X = self._filtering_stage_basl(accepts_x=current_train_X, rejects_x=current_unlabeled_X)
        if not self.silent: print(f"Filtering rejects: Kept {len(current_unlabeled_X)} samples.")

        # Initialize iteration tracking and model storage
        iter_count = 0
        perf_vector = []
        fitted_models_at_iterations = []

        # Iterative self-learning loop
        while True:
            iter_count += 1 # Increment iteration count

            # Check stopping criteria (jmax, Xr != empty, Vj >= Vj-1)
            if iter_count > self.max_iterations: # Max iterations reached
                break
            if current_unlabeled_X.empty: # No more unlabeled data
                if not self.silent: print("No unlabeled samples remaining, ending iterations.")
                break

            # Early stopping check based on performance (Vj >= Vj-1)
            # This applies only if early_stop is true AND we have at least two performance points to compare.
            if self.early_stop and len(perf_vector) >= 1 and perf_vector[-1] < perf_vector[-2] if len(perf_vector) >= 2 else False:
                if not self.silent: print(f"--- Iteration {iter_count}: Early stopping (performance dropped). ---")
                break

            if not self.silent: print(f"\n--- Iteration {iter_count} ---")

            # Labeling stage: pseudo-label rejects, update training data (pseudo-code 5-7)
            current_train_X, current_train_y, current_unlabeled_X, num_pseudo_labeled = \
                self._labeling(current_train_X, current_train_y, current_unlabeled_X, iteration_number=iter_count)

            # Training stage: train strong model on augmented data
            current_strong_model_iter = copy.deepcopy(self.strong_estimator)
            current_strong_model_iter.fit(current_train_X, current_train_y)
            fitted_models_at_iterations.append(current_strong_model_iter)

            # Evaluation stage: assess current model performance
            current_perf = -np.inf
            if self.early_stop and not holdout_x_accept.empty and not holdout_x_reject.empty:
                y_pred_acc_holdout = current_strong_model_iter.predict(holdout_x_accept)
                y_proba_acc_holdout = current_strong_model_iter.predict_proba(holdout_x_accept)[:, 1]
                y_pred_rej_holdout = current_strong_model_iter.predict(holdout_x_reject)
                y_proba_rej_holdout = current_strong_model_iter.predict_proba(holdout_x_reject)[:, 1]

                current_perf = self.bayesian_evaluator.BM(
                    y_true_acc=holdout_y_accept.values,
                    y_proba_acc=y_proba_acc_holdout,
                    y_proba_rej=y_proba_rej_holdout,
                    rejects_prior=y_proba_rej_holdout,
                    acc_rate=self.bayesian_acc_rate,
                    seed=7
                )

            perf_vector.append(current_perf)

            if not self.silent and self.early_stop:
                print(f"Iteration {iter_count} strong learner performance: {perf_vector[-1]:.4f}")
                # Log if performance decreased for debugging, actual break handled by while condition
                if len(perf_vector) >=2 and perf_vector[-1] < perf_vector[-2]:
                    print('-- Performance decreased.')

        # Final model selection based on performance
        if self.early_stop and len(perf_vector) > 0:
            best_iter_index = np.argmax(perf_vector)
            self.model_ = fitted_models_at_iterations[best_iter_index]
            if not self.silent:
                # Adjust index for 1-based iteration printing
                print(f"Final model selected from iteration {best_iter_index + 1} (BM: {perf_vector[best_iter_index]:.4f}).")
        else:
            # If no iterations ran, or early stopping wasn't active, use the last fitted model.
            self.model_ = fitted_models_at_iterations[-1] if fitted_models_at_iterations else copy.deepcopy(self.strong_estimator)
            if not hasattr(self.model_, 'n_features_in_'): # Check if fitted
                self.model_.fit(current_train_X, current_train_y)
            if not self.silent and not self.early_stop:
                print("Final model from last iteration (early stopping off).")

        # Post-process inferred labels
        self.inferred_labels_ = self.inferred_labels_.replace({-1: np.nan})

        if not self.silent: print('-- Finished fitting BiasAwareSelfLearning.')
        return self


    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Predict class labels for samples in X using the fitted BiasAwareSelfLearning model.

        This method uses the 'strong' model which is the final or best-performing model trained during the self-learning process, to make predictions
        on new, unseen data.

        Parameters
        ----------
        X : np.ndarray or pd.DataFrame
            The input samples (features) for which to predict class labels.

        Returns
        -------
        np.ndarray
            Predicted class labels (0 or 1).

        Raises
        ------
        sklearn.exceptions.NotFittedError
            If the `fit` method has not been called (i.e., the model is not trained yet).
        """
        # Check if the model has been fitted (i.e., self.model_ exists).
        if not hasattr(self, 'model_'):
            raise NotFfittedError("This estimator instance is not fitted yet. "
                                 "Call 'fit' with appropriate arguments before using this method.")
        # Delegate the prediction to the internal fitted strong learner.
        return self.model_.predict(X)

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """
        Predict class probabilities for samples in X using the fitted model.

        This method delegates the `predict_proba` call to the internally
        stored strong learner (`self.model_`) which was selected as the
        final model during the `fit` process.

        Parameters
        ----------
        X : np.ndarray
            The input samples for which to predict class probabilities.
            This should be a numpy array or a pandas DataFrame compatible
            with the strong estimator's input requirements.

        Returns
        -------
        np.ndarray
            The predicted class probabilities for each sample.
            The shape of the array will be (n_samples, n_classes),
            where n_classes is typically 2 for binary classification
            (probability of class 0, probability of class 1).

        Raises
        ------
        sklearn.exceptions.NotFittedError
            If the `fit` method has not been called on this estimator instance
            before `predict_proba` is called.
        AttributeError
            If the strong estimator assigned to `self.model_` does not have
            a `predict_proba` method, which is a requirement for this class.
            eg if it is a regression model or clustering model.
        """

        if not hasattr(self, 'model_'):
            raise NotFittedError("This estimator instance is not fitted yet. "
                                 "Call 'fit' with appropriate arguments before using this method.")
        if not hasattr(self.model_, 'predict_proba'):
            raise AttributeError(f"The strong estimator '{self.strong_estimator.__class__.__name__}' "
                                 "does not support `predict_proba` method.")
        return self.model_.predict_proba(X)

class LabelAllRejectsAsBad(RejectInference):
    """
    Reject Inference Benchmark model that labels all rejected samples as BAD (1)
    and trains a strong estimator on the combined data.

    """
    def __init__(self, strong_estimator: BaseEstimator, silent: bool = False):
        """
        Parameters
        ----------
        strong_estimator : BaseEstimator
            A scikit-learn compatible classifier that implements `fit`, `predict`, and `predict_proba`.

        silent : bool, default=False
            If True, suppresses verbose output during fitting.
        """
        super().__init__()

        self.strong_estimator = strong_estimator
        self.silent = silent
        self.model_ = None

        # Parameter validations
        if self.strong_estimator is None:
            raise ValueError("`strong_estimator` cannot be None.")
        if not hasattr(self.strong_estimator, 'fit'):
            raise AttributeError("`strong_estimator` must have a `fit` method.")
        if not hasattr(self.strong_estimator, 'predict_proba'):
            raise AttributeError("`strong_estimator` must have a `predict_proba` method.")
        if not isinstance(self.silent, bool):
            raise ValueError("`silent` must be a boolean.")

    def fit(self, accepts_x: pd.DataFrame, accepts_y: pd.Series, rejects_x: pd.DataFrame):
        """
        Fit the model by labelling all rejects as BAD (1) and training a strong estimator on the combined data.

        Parameters
        ----------
        accepts_x : pd.DataFrame
            Features of the accepted (labeled) applications.
        accepts_y : pd.Series
            Labels of the accepted applications (0 = GOOD, 1 = BAD).
        rejects_x : pd.DataFrame
            Features of the rejected applications (unlabeled).

        Returns
        -------
        self : object
            The fitted instance.
        """
        # Input validations
        if not isinstance(accepts_x, pd.DataFrame):
            raise TypeError("`accepts_x` must be a pandas DataFrame.")
        if not isinstance(accepts_y, pd.Series):
            raise TypeError("`accepts_y` must be a pandas Series.")
        if not isinstance(rejects_x, pd.DataFrame):
            raise TypeError("`rejects_x` must be a pandas DataFrame.")
        if len(accepts_x) != len(accepts_y):
            raise ValueError("`accepts_x` and `accepts_y` must have the same length.")
        if accepts_x.empty or accepts_y.empty:
            raise ValueError("`accepts_x` and `accepts_y` cannot be empty.")
        if rejects_x.empty:
            raise ValueError("`rejects_x` cannot be empty.")
        if not accepts_x.columns.equals(rejects_x.columns):
            raise ValueError("Columns of `rejects_x` must match `accepts_x`.")

        # Label all rejects as BAD
        y_rejects = pd.Series(1, index=rejects_x.index)

        # Combine accepts and rejects
        X_train = pd.concat([accepts_x, rejects_x], axis=0)
        y_train = pd.concat([accepts_y, y_rejects], axis=0)

        if not self.silent:
            print(f"Training on {len(X_train)} samples: {len(accepts_x)} accepts + {len(rejects_x)} rejects (all labeled BAD).")

        # Fit strong estimator
        self.strong_estimator.fit(X_train, y_train)
        self.model_ = self.strong_estimator

        if not self.silent:
            print("LabelAllRejectsAsBad model training complete.")

        return self

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        Predict class labels using the fitted model.

        Parameters
        ----------
        X : pd.DataFrame
            Feature matrix for prediction.

        Returns
        -------
        y_pred : np.ndarray
            Predicted class labels (0 or 1).
        """
        if self.model_ is None:
            raise RuntimeError("Model is not fitted. Call `fit` first.")
        return self.model_.predict(X)

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """
        Predict class probabilities using the fitted model.

        Parameters
        ----------
        X : pd.DataFrame
            Feature matrix for prediction.

        Returns
        -------
        y_proba : np.ndarray
            Predicted probabilities.
        """
        if self.model_ is None:
            raise RuntimeError("Model is not fitted. Call `fit` first.")
        return self.model_.predict_proba(X)

class Reweighting(RejectInference):
    """
    Reject Inference benchmark model using re-weighting.
    Trains only on accepted applications but applies sample weights
    to correct for selection bias based on acceptance probabilities.
    """

    def __init__(self, strong_estimator: BaseEstimator, acceptance_model: BaseEstimator = None,
                 banded: bool = False, n_bands: int = 10, random_state: int = 42, silent: bool = False):
        """
        Parameters
        ----------
        strong_estimator : BaseEstimator
            Scikit-learn compatible classifier for the final credit scoring model.

        acceptance_model : BaseEstimator, optional
            Classifier to estimate p(accepted|x). Defaults to LogisticRegression() if None.

        banded : bool, default=False
            Whether to use banded weights (group-based averaging).

        n_bands : int, default=10
            Number of bands for banded weights if banded=True.

        random_state : int, default=42
            Random state for reproducibility.

        silent : bool, default=False
            If True, suppresses verbose output.
        """
        super().__init__()
        self.strong_estimator = strong_estimator
        self.acceptance_model = acceptance_model if acceptance_model is not None else \
            LogisticRegression(max_iter=1000, random_state=random_state)
        self.banded = banded
        self.n_bands = n_bands
        self.random_state = random_state
        self.silent = silent
        self.model_ = None

        # Validations
        if not hasattr(self.strong_estimator, 'fit'):
            raise AttributeError("`strong_estimator` must implement `fit`.")
        if not hasattr(self.acceptance_model, 'fit'):
            raise AttributeError("`acceptance_model` must implement `fit`.")

    def fit(self, accepts_x: pd.DataFrame, accepts_y: pd.Series, rejects_x: pd.DataFrame):
        """
        Fit the model using re-weighting.

        Parameters
        ----------
        accepts_x : pd.DataFrame
            Features of accepted (labeled) applications.
        accepts_y : pd.Series
            Labels of accepted applications.
        rejects_x : pd.DataFrame
            Features of rejected (unlabeled) applications.

        Returns
        -------
        self : object
        """
        # Type checks
        if not isinstance(accepts_x, pd.DataFrame) or not isinstance(rejects_x, pd.DataFrame):
            raise TypeError("accepts_x and rejects_x must be DataFrames.")
        if not isinstance(accepts_y, pd.Series):
            raise TypeError("accepts_y must be a Series.")
        if not accepts_x.columns.equals(rejects_x.columns):
            raise ValueError("Columns of accepts_x and rejects_x must match.")

        # Train acceptance model
        if not self.silent:
            print("Training acceptance model to estimate p(accepted|x)...")
        acceptance_labels = pd.concat([
            pd.Series(1, index=accepts_x.index),
            pd.Series(0, index=rejects_x.index)
        ])
        acceptance_features = pd.concat([accepts_x, rejects_x], axis=0)
        self.acceptance_model.fit(acceptance_features, acceptance_labels)

        # Compute acceptance probabilities
        accept_probs = self.acceptance_model.predict_proba(accepts_x)[:, 1]
        accept_probs = np.clip(accept_probs, 1e-6, 1.0)

        # Compute sample weights
        if self.banded:
            if not self.silent:
                print(f"Computing banded sample weights ({self.n_bands} bands)...")
            bands = pd.qcut(accept_probs, q=self.n_bands, labels=False, duplicates="drop")
            mean_weights = (1.0 / pd.Series(accept_probs)).groupby(bands).transform('mean')
            sample_weights = mean_weights.values
        else:
            if not self.silent:
                print("Computing sample weights for accepted applications...")
            sample_weights = 1.0 / accept_probs

        # Normalize weights to mean=1
        sample_weights /= np.mean(sample_weights)

        # Train strong estimator with weights
        if not self.silent:
            print("Training strong estimator with computed weights...")
        self.strong_estimator.fit(accepts_x, accepts_y, sample_weight=sample_weights)
        self.model_ = self.strong_estimator

        if not self.silent:
            print("Reweighting model training complete.")
        return self

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """Predict class labels."""
        if self.model_ is None:
            raise RuntimeError("Model not fitted.")
        return self.model_.predict(X)

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """Predict class probabilities."""
        if self.model_ is None:
            raise RuntimeError("Model not fitted.")
        return self.model_.predict_proba(X)

class HeckmanBivariate(RejectInference):
    """
    A custom Bivariate Probit model that uses a manual log-likelihood function
    and Scipy's optimizer. This allows for separate feature sets for the
    selection and outcome equations, a more advanced and flexible approach.
    """
    def __init__(self, selection_features: Union[List[int], None] = None, outcome_features: Union[List[int], None] = None):
        """
        Initializes the model and stores feature indices for selection and outcome equations.
        These will be populated after the model is fitted.
        """
        super().__init__()
        self.outcome_coef_ = None
        self.selection_coef_ = None
        self.correlation_ = None
        self.selection_features = selection_features
        self.outcome_features = outcome_features
        self.outcome_feature_count = None
        self.selection_feature_count = None

    @staticmethod
    def _unpack_params(params, n_outcome, n_selection):
        """
        Unpacks a flat array of all parameters into separate coefficient vectors
        for the outcome and selection equations, and the correlation.
        The correlation is bounded between -1 and 1 using the hyperbolic tangent (tanh).
        """
        outcome_coef = params[:n_outcome]
        selection_coef = params[n_outcome:n_outcome + n_selection]
        rho_unbounded = params[n_outcome + n_selection]
        correlation = np.tanh(rho_unbounded)
        return outcome_coef, selection_coef, correlation

    def _neg_log_likelihood(self, params, endog, exog_outcome, exog_selection):
        outcome_labels = endog[:, 0]
        selection_outcomes = endog[:, 1]
        n_samples = endog.shape[0]

        n_outcome = self.outcome_feature_count
        n_selection = self.selection_feature_count

        outcome_coef, selection_coef, correlation = self._unpack_params(params, n_outcome, n_selection)

        correlation = np.clip(correlation, -0.9999, 0.9999)

        selection_index = exog_selection @ selection_coef
        outcome_index = exog_outcome @ outcome_coef

        mask_selected = selection_outcomes.astype(bool)
        mask_y1 = mask_selected & (outcome_labels == 1)
        mask_y0 = mask_selected & (outcome_labels == 0)
        mask_not_selected = ~mask_selected

        ll = np.zeros(n_samples)

        if mask_y1.any():
            try:
                pts = np.column_stack([selection_index[mask_y1], outcome_index[mask_y1]])
                cov = np.array([[1.0, correlation], [correlation, 1.0]])
                cdf_vals = multivariate_normal.cdf(pts, mean=[0, 0], cov=cov)
                ll[mask_y1] = np.log(np.clip(cdf_vals, 1e-15, 1.0))
            except (np.linalg.LinAlgError, ValueError):
                return np.inf

        if mask_y0.any():
            try:
                pts = np.column_stack([selection_index[mask_y0], -outcome_index[mask_y0]])
                cov = np.array([[1.0, -correlation], [-correlation, 1.0]])
                cdf_vals = multivariate_normal.cdf(pts, mean=[0, 0], cov=cov)
                ll[mask_y0] = np.log(np.clip(cdf_vals, 1e-15, 1.0))
            except (np.linalg.LinAlgError, ValueError):
                return np.inf

        if mask_not_selected.any():
            cdf_vals = norm.cdf(-selection_index[mask_not_selected], loc=0, scale=1)
            ll[mask_not_selected] = np.log(np.clip(cdf_vals, 1e-15, 1.0))

        log_likelihood = np.sum(ll)

        if np.isnan(log_likelihood) or np.isinf(log_likelihood) or log_likelihood > 0:
            return np.inf

        return -log_likelihood

    def fit(self, accepts_x: pd.DataFrame, accepts_y: pd.Series, rejects_x: pd.DataFrame):

        """
        Fits the Bivariate Probit model by running the optimization process.
        """
        print("Starting Bivariate Probit Model fitting...")
        # Ignore holdout_x parameter to match abstract base class signature
        
        # 1. Prepare all data for the optimizer
        X_all = np.vstack([accepts_x, rejects_x])
        is_accepted = np.hstack([np.ones(accepts_x.shape[0]), np.zeros(rejects_x.shape[0])])
        y_all = np.hstack([accepts_y, np.full(rejects_x.shape[0], np.nan)])

        self.selection_features_idx = self.selection_features if self.selection_features else list(range(X_all.shape[1]))
        self.outcome_features_idx = self.outcome_features if self.outcome_features else list(range(X_all.shape[1]))

        self.outcome_feature_count = len(self.outcome_features_idx) + 1
        self.selection_feature_count = len(self.selection_features_idx) + 1

        endog = np.column_stack([y_all, is_accepted])
        exog_outcome = np.hstack([np.ones((X_all.shape[0], 1)), X_all[:, self.outcome_features_idx]])
        exog_selection = np.hstack([np.ones((X_all.shape[0], 1)), X_all[:, self.selection_features_idx]])

        # 2. Get better initial parameter estimates from simpler models
        try:
            selection_model = sm.Probit(is_accepted, exog_selection).fit(disp=False, method='bfgs')
            accepted_mask = is_accepted.astype(bool)
            outcome_model = sm.Probit(y_all[accepted_mask], exog_outcome[accepted_mask]).fit(disp=False, method='bfgs')

            # Combine the results and add a guess for the correlation
            initial_params = np.concatenate([
                outcome_model.params,
                selection_model.params,
                [0.0]
            ])
        except Exception as e:
            print(f"Could not fit simpler models for initialization: {e}. Falling back to zeros.")
            initial_params = np.zeros(self.outcome_feature_count + self.selection_feature_count + 1)

        # 3. Minimize the negative log-likelihood using a more efficient optimizer
        try:
            result = minimize(
                self._neg_log_likelihood,
                initial_params,
                args=(endog, exog_outcome, exog_selection),
                method='L-BFGS-B',
                options={'disp': True, 'maxiter': 1000}
            )

            if result.success:
                print("Optimization successful.")
                self.outcome_coef_, self.selection_coef_, self.correlation_ = self._unpack_params(
                    result.x, self.outcome_feature_count, self.selection_feature_count)
            else:
                print(f"Optimization failed: {result.message}")
                raise ValueError("Optimizer failed to converge.")
        except Exception as e:
            print(f"Error during model fitting: {e}")
            raise e

        print("Bivariate Probit Model fitting complete.")
        return self

    def predict_proba(self, X: np.ndarray) -> np.ndarray:

        """
        Predicts the probability of the positive class for new data using the fitted model.
        """
        if self.outcome_coef_ is None or self.selection_coef_ is None or self.correlation_ is None:
            print("Model has not been fitted yet.")
            return np.zeros(X.shape[0])

        # Prepare the new data with the same features and intercepts as used for fitting
        exog_outcome = np.hstack([np.ones((X.shape[0], 1)), X[:, self.outcome_features_idx]])

        # Calculate the "index" or latent variable for the outcome equation
        outcome_index = exog_outcome @ self.outcome_coef_

        # The final probability is the CDF of the outcome index, which is the
        # probability of the outcome being 1, regardless of the selection.
        # Use norm.cdf for univariate normal CDF
        cdf_vals = norm.cdf(outcome_index, loc=0, scale=1)

        return cdf_vals

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Predicts the class labels (0 or 1) for new data.
        """
        probas = self.predict_proba(X)
        return (probas > 0.5).astype(int)
    
class HeckmanTwoStage(RejectInference):
    """
    Implements a flexible two-stage Heckman-style model for reject inference.
    The first stage models the selection process (acceptance), and the
    second stage models the outcome, using a correction term from the
    first stage to account for selection bias. This version allows
    for a choice of classifiers (Probit or XGBoost) in both stages.
    """
    def __init__(self, selection_classifier='Probit', outcome_classifier='Probit', selection_features: Union[List[int], None] = None, outcome_features: Union[List[int], None] = None):
        """
        Initializes the model with specified classifiers for each stage and feature lists.

        Args:
            selection_classifier (str): The classifier for the selection stage ('Probit' or 'XGB').
            outcome_classifier (str): The classifier for the outcome stage ('Probit' or 'XGB').
            selection_features (list, optional): A list of column indices for the selection equation.
            outcome_features (list, optional): A list of column indices for the outcome equation.
        """
        # Ensure super().__init__() is called if the parent class has an __init__
        super().__init__()
        
        if selection_classifier not in ['Probit', 'XGB'] or outcome_classifier not in ['Probit', 'XGB']:
            raise ValueError("Classifiers must be either 'Probit' or 'XGB'.")

        self.selection_classifier = selection_classifier
        self.outcome_classifier = outcome_classifier
        self.selection_model = None
        self.outcome_model = None
        self.selection_features = selection_features
        self.outcome_features = outcome_features
    
    def fit(self, accepts_x: pd.DataFrame, accepts_y: pd.Series, rejects_x: pd.DataFrame):
        """
        Fits the two-stage model.
        
        This method now matches the signature of the abstract base class.
        The `selection_features` and `outcome_features` are now accessed from
        the instance variables set during initialization.
        """
        print("Starting Two-Stage Model fitting...")
        
        # --- Stage 1: The Selection Equation ---
        print("Stage 1: Fitting selection model on entire population...")
        
        X_all = np.vstack([accepts_x, rejects_x])
        is_accepted = np.hstack([np.ones(accepts_x.shape[0]), np.zeros(rejects_x.shape[0])])
        
        # Access features from the __init__ parameters
        self.selection_features_idx = self.selection_features if self.selection_features else list(range(X_all.shape[1]))
        self.outcome_features_idx = self.outcome_features if self.outcome_features else list(range(X_all.shape[1]))
        
        X_selection = X_all[:, self.selection_features_idx]
        
        # Fit the specified selection classifier
        if self.selection_classifier == 'Probit': #generalized linear model
            first_stage_exog = sm.add_constant(X_selection, prepend=False)
            self.selection_model = sm.Probit(is_accepted, first_stage_exog).fit(disp=0)
        elif self.selection_classifier == 'XGB':
            self.selection_model = XGBClassifier(eval_metric='logloss')
            self.selection_model.fit(X_selection, is_accepted)
        
        print("Stage 1 complete.")
        
        # --- Stage 2: The Outcome Equation with Bias Correction ---
        print("Stage 2: Fitting outcome model on accepted population...")
        #contains only the features from the accepted applicants that were used to train the selection model
        X_selection_accepted = X_all[:accepts_x.shape[0], self.selection_features_idx]
        
        # Calculate the Inverse Mills Ratio (IMR) based on the Stage 1 model
        if self.selection_classifier == 'Probit':
            z_accepted = self.selection_model.predict(sm.add_constant(X_selection_accepted, prepend=False))
        elif self.selection_classifier == 'XGB':
            # Get the predicted probabilities from the XGBoost model
            probas_accepted = self.selection_model.predict_proba(X_selection_accepted)[:, 1]
            # Convert probabilities to a Z-score (linear predictor)
            # Clip values to avoid issues with log(0)
            probas_accepted = np.clip(probas_accepted, 1e-15, 1 - 1e-15)
            z_accepted = norm.ppf(probas_accepted)
        
        # IMR calculation is consistent regardless of Stage 1 classifier
        imr = norm.pdf(z_accepted) / norm.cdf(z_accepted)
        
        X_outcome = accepts_x[:, self.outcome_features_idx]
        second_stage_exog = np.hstack([X_outcome, imr.reshape(-1, 1)])
        
        # Fit the specified outcome classifier
        if self.outcome_classifier == 'Probit':
            second_stage_exog_sm = sm.add_constant(second_stage_exog, prepend=False)
            self.outcome_model = sm.Probit(accepts_y, second_stage_exog_sm).fit(disp=0)
        elif self.outcome_classifier == 'XGB':
            self.outcome_model = XGBClassifier(eval_metric='logloss')
            self.outcome_model.fit(second_stage_exog, accepts_y)
            
        print("Stage 2 complete.")
        print("Two-Stage Model fitting complete.")
        return self

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """
        Predicts the probability of the positive class for new data using the
        two-stage model's parameters.
        """
        if self.selection_model is None or self.outcome_model is None:
            print("Model has not been fitted yet.")
            return np.zeros(X.shape[0])

        # First, calculate the IMR for the new data
        X_selection_new = X[:, self.selection_features_idx]
        if self.selection_classifier == 'Probit':
            z_new = self.selection_model.predict(sm.add_constant(X_selection_new, prepend=False))
        elif self.selection_classifier == 'XGB':
            probas_new = self.selection_model.predict_proba(X_selection_new)[:, 1]
            probas_new = np.clip(probas_new, 1e-15, 1 - 1e-15)
            z_new = norm.ppf(probas_new)

        imr_new = norm.pdf(z_new) / norm.cdf(z_new)

        # Then, use the outcome model to predict probabilities
        X_outcome_new = X[:, self.outcome_features_idx]
        final_exog = np.hstack([X_outcome_new, imr_new.reshape(-1, 1)])

        if self.outcome_classifier == 'Probit':
            final_exog_sm = sm.add_constant(final_exog, prepend=False)
            probas = self.outcome_model.predict(final_exog_sm)
        elif self.outcome_classifier == 'XGB':
            probas = self.outcome_model.predict_proba(final_exog)[:, 1]

        return probas

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Predicts the class labels (0 or 1) for new data.
        """
        probas = self.predict_proba(X)
        return (probas > 0.5).astype(int)